{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Tensorflow...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fileinput\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Initiating Tensorflow...\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke types in order\n",
    "STROKE_TYPE = [\"Serve\",\n",
    "            \"Forehand\",\n",
    "            \"Backhand\",\n",
    "            \"NoStroke\"]\n",
    "\n",
    "# Stroke type indices\n",
    "SERVE       = 0\n",
    "FOREHAND    = 1\n",
    "BACKHAND    = 2\n",
    "NOSTROKE    = 3\n",
    "\n",
    "# Video Keypoint locations\n",
    "SERVE_OUTPUT_LOC    = f\"videos/{STROKE_TYPE[SERVE]}/output/\"\n",
    "FOREHAND_OUTPUT_LOC = f\"videos/{STROKE_TYPE[FOREHAND]}/output/\"\n",
    "BACKHAND_OUTPUT_LOC = f\"videos/{STROKE_TYPE[BACKHAND]}/output/\"\n",
    "NOSTROKE_OUTPUT_LOC = f\"videos/{STROKE_TYPE[NOSTROKE]}/output/\"\n",
    "\n",
    "# Directory to save graphed distances for each stroke based on keypoints\n",
    "PLT_FILE_PATH       = \"images/\"\n",
    "\n",
    "# Frames in a video\n",
    "FRAMES_PER_SAMPLE   = 58\n",
    "\n",
    "# Required Mediapipe pose keypoints\n",
    "# Reference: https://github.com/google/mediapipe/blob/master/docs/solutions/pose.md\n",
    "LEFT_SHOULDER   = 11\n",
    "RIGHT_SHOULDER  = 12\n",
    "LEFT_HAND       = 15\n",
    "RIGHT_HAND      = 16\n",
    "LEFT_HIP        = 23 \n",
    "RIGHT_HIP       = 24\n",
    "LEFT_ANKLE      = 27\n",
    "RIGHT_ANKLE     = 28\n",
    "\n",
    "DISTANCES_TYPE = [\"RH to C\", \n",
    "                \"LH to C\", \n",
    "                \"RH to RA\", \n",
    "                \"LH to LA\",\n",
    "                \"RH to LH\",\n",
    "                \"RH to LHIP\",\n",
    "                \"LH to RHIP\",\n",
    "                \"MHIP-X to RH-X\",\n",
    "                \"MHIP-X to LH-X\"]\n",
    "\n",
    "# Each keypoint is of the form [ID, X, Y]\n",
    "ID      = 0\n",
    "X_COORD = 1\n",
    "Y_COORD = 2\n",
    "\n",
    "# Model Parameters\n",
    "RANDOM_SEED         = 42\n",
    "TEST_PERCENT        = 0.2   # Percent for Testing\n",
    "\n",
    "# Outputs from LSTM\n",
    "# Also used for hidden count\n",
    "LSTM_OP_SPACE       = 100\n",
    "\n",
    "DROPOUT_FACTOR      = 0.2\n",
    "MODEL_EPOCH_COUNT   = 50\n",
    "MODEL_BATCH_SIZE    = 32\n",
    "SHOULD_GEN_GRAPH    = True  # Generate graphs for each frame?\n",
    "MODEL_NAME          = \"models/tf_simple_lstm_model\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each type has dimensions: video_num, frame_num, key_num\n",
    "# Read the text file\n",
    "def getRows(location):\n",
    "    # Get all files in the folder\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "    \n",
    "    videos = []\n",
    "    # Iterate over each file and read its lines\n",
    "    for file_path in file_list:     # Parse all the videos\n",
    "        print(f\"File: {file_path}\")\n",
    "        frames = []\n",
    "        for line in fileinput.input(file_path): # Parse all the frames\n",
    "            i = 0\n",
    "            row = line.split()\n",
    "            row = [int(value) for value in row]\n",
    "            keypoints = []\n",
    "            while i < len(row): # Parse all the keypoints\n",
    "                keypoint = []\n",
    "                keypoint.append(row[i])\n",
    "                # print(row[i])\n",
    "                keypoint.append(row[i+1])\n",
    "                keypoint.append(row[i+2])\n",
    "                # print(str(keypoint) + '\\n')\n",
    "                keypoints.append(keypoint) \n",
    "                i += 3\n",
    "        \n",
    "            frames.append(keypoints)\n",
    "        fileinput.close()\n",
    "        videos.append(frames)\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points p0 and p1 are in the form [0:pointNum 1:xCoord 2:yCoord]\n",
    "def distance(p0, p1):\n",
    "    return math.sqrt(\n",
    "        (p1[X_COORD] - p0[X_COORD]) ** 2 + \n",
    "        (p1[Y_COORD] - p0[Y_COORD]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a directory if it doesn't exist\n",
    "# os.listdir()\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory Created!: {directory}\")\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This where the model building happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    STROKE_COUNT = len(STROKE_TYPE)\n",
    "    DISTANCES_COUNT  = len(DISTANCES_TYPE)\n",
    "\n",
    "    # construct the argument parser and parse the arguments\n",
    "    # ap = argparse.ArgumentParser()\n",
    "    # ap.add_argument(\"-g\", \"--graphs\", type=int, default=0,\n",
    "    #     help=\"Set as 0 if you dont want to generate graphs for the videos\")\n",
    "    # ap.add_argument(\"-o\", \"--optimize\", type=int, default=0,\n",
    "    #     help=\"Set as 0 if you dont want to optimize the model for mobile devices\")\n",
    "    # args = vars(ap.parse_args())\n",
    "\n",
    "    # SHOULD_GEN_GRAPH = args[\"graphs\"]\n",
    "    # SHOULD_MOBILE_OPTIMIZE = args[\"optimize\"]\n",
    "\n",
    "\n",
    "    print(\"Reading Keypoint files...\")\n",
    "\n",
    "    # Input keypoints for all videos (30 serves, 30 forehands, 30 backhands and 60 no stroke play) with 30 FPS Recordings\n",
    "    serves      = getRows(SERVE_OUTPUT_LOC)\n",
    "    forehands   = getRows(FOREHAND_OUTPUT_LOC)\n",
    "    backhands   = getRows(BACKHAND_OUTPUT_LOC)\n",
    "    nostrokes   = getRows(NOSTROKE_OUTPUT_LOC)\n",
    "\n",
    "    dataset = []\n",
    "    dataset.append(serves)\n",
    "    dataset.append(forehands)\n",
    "    dataset.append(backhands)\n",
    "    dataset.append(nostrokes)\n",
    "\n",
    "    # Color the distances\n",
    "    cmap = plt.get_cmap(\"rainbow\", DISTANCES_COUNT)  \n",
    "    # VIOLET        - RH to C\n",
    "    # BLUE          - LH to C\n",
    "    # LIGHT BLUE    - RH to RA\n",
    "    # CYAN          - LH to LA\n",
    "    # LIGHT GREEN   - RH to LH\n",
    "    # GREEN         - RH to LHIP\n",
    "    # LIGHT ORANGE  - LH to RHIP\n",
    "    # ORANGE        - MHIP-X to RH-X\n",
    "    # RED           - MHIP-X to LH-X\n",
    "\n",
    "    input_set = []\n",
    "    label_per_video = []\n",
    "    print(\"Calculating Distances...\\n\")\n",
    "    # For each set of keypoints in a video in all videos\n",
    "    for i in range(len(dataset)):\n",
    "        j = 0\n",
    "        for video in dataset[i]:\n",
    "            sequence = []\n",
    "            k = 0\n",
    "            for frame in video:\n",
    "                distances = []\n",
    "                # Calculate the 9 distances as follows:\n",
    "                chest_point = [-1, \n",
    "                            (frame[LEFT_SHOULDER][X_COORD]+\n",
    "                            frame[RIGHT_SHOULDER][X_COORD])/2,  \n",
    "                            (frame[LEFT_SHOULDER][Y_COORD]+\n",
    "                            frame[RIGHT_SHOULDER][Y_COORD])/2]\n",
    "                hip_center = [-1, \n",
    "                            (frame[LEFT_HIP][X_COORD]+\n",
    "                            frame[RIGHT_HIP][X_COORD])/2,  \n",
    "                            (frame[LEFT_HIP][Y_COORD]+\n",
    "                            frame[RIGHT_HIP][Y_COORD])/2]\n",
    "                # torso length is the distance between shoulder mid point and hip mid\n",
    "                torso_length = distance(chest_point, hip_center)\n",
    "                if torso_length == 0:\n",
    "                    print(f\"\\n\\nTorso Length 0 for {STROKE_TYPE[i]}_video_{j+1} Frame: {k+1}?\\nChest Point: {chest_point}, Hip Center: {hip_center}\\nDefaulting Torso Length to 1\")\n",
    "                    print(f\"Left Shoulder: {frame[LEFT_SHOULDER]}\")\n",
    "                    print(f\"Right Shoulder: {frame[RIGHT_SHOULDER]}\")\n",
    "                    print(f\"Left Hip: {frame[LEFT_HIP]}\")\n",
    "                    print(f\"Right Hip: {frame[RIGHT_HIP]}\")\n",
    "                    torso_length = 1\n",
    "                # Normalize all the distances based on torso length and append them to the distance array\n",
    "                # Dominant hand to chest \n",
    "                distances.append(distance(\n",
    "                                    frame[RIGHT_HAND], chest_point)/torso_length)\n",
    "                # Non-dominant hand to chest\n",
    "                distances.append(distance(\n",
    "                                    frame[LEFT_HAND], chest_point)/torso_length)\n",
    "                # Dominant hand to dominant side foot\n",
    "                distances.append(distance(\n",
    "                                    frame[RIGHT_HAND], frame[RIGHT_ANKLE])/torso_length)\n",
    "                # Non-dominant hand to non-dominant hand side foot\n",
    "                distances.append(distance(frame[LEFT_HAND], frame[LEFT_ANKLE])/torso_length)\n",
    "                # Hand to hand \n",
    "                distances.append(distance(frame[LEFT_HAND], frame[RIGHT_HAND])/torso_length)\n",
    "                # Dominant hand to nondominant side hip \n",
    "                distances.append(distance(frame[RIGHT_HAND], frame[LEFT_HIP])/torso_length)\n",
    "                # Non-Dominant hand to dominant side hip\n",
    "                distances.append(distance(frame[LEFT_HAND], frame[RIGHT_HIP])/torso_length)\n",
    "                # Body (Hip Center) to dominant hand x-axis distance\n",
    "                distances.append(abs(\n",
    "                                hip_center[X_COORD]-frame[RIGHT_HAND][X_COORD])/torso_length)\n",
    "                # Body (Hip Center) to non-dominant hand x-axis distance \n",
    "                distances.append(abs(\n",
    "                                hip_center[X_COORD]-frame[LEFT_HAND][X_COORD])/torso_length)\n",
    "                # Store in a data set array\n",
    "                sequence.append(distances)\n",
    "                k+=1\n",
    "            # print(f\"Generate Graphs: {SHOULD_GEN_GRAPH}\")\n",
    "            if SHOULD_GEN_GRAPH != 0:  \n",
    "                fig, ax = plt.subplots()\n",
    "                arr = np.array(sequence)\n",
    "                # Iterate over each distance column and plot with a different color\n",
    "                for l in range(DISTANCES_COUNT):\n",
    "                    ax.plot(arr[:,l], color=cmap(l), label=f\"{DISTANCES_TYPE[l]}\")    \n",
    "\n",
    "                # Set the title and labels\n",
    "                ax.set_title(f\"Distances for {STROKE_TYPE[i]}, Video {j+1}\")\n",
    "                ax.set_xlabel(\"Frames\")\n",
    "                ax.set_ylabel(\"Distance\")\n",
    "\n",
    "                # Add a legend outside the graph\n",
    "                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                # Create images directory\n",
    "                create_dir(PLT_FILE_PATH)\n",
    "                # Save graph path\n",
    "                filename = f\"{PLT_FILE_PATH}{STROKE_TYPE[i]}_video_{j+1}_distances.png\"\n",
    "                print(f\"Saved graph: {filename}\")\n",
    "                plt.savefig(filename, bbox_inches='tight')\n",
    "                # Close the figure to free up resources\n",
    "                plt.close(fig)\n",
    "            \n",
    "            input_set.append(sequence)\n",
    "            j+=1\n",
    "            # Label the video respectively as (0: Serve, 1: Forehand, 2: Backhand, 3: No Stroke) \n",
    "            label_per_video.append(i)\n",
    "\n",
    "    SAMPLE_COUNT = len(input_set) # Store total video count\n",
    "    # print(\"Inputs:\\n\", SAMPLE_COUNT)\n",
    "    # print(\"\\n\\nLabels:\\n\",label_per_video)\n",
    "\n",
    "    # Reshape the data matrix: Assuming you have a data matrix called data with shape (SAMPLE_COUNT, FRAMES_PER_SAMPLE, DISTANCES_COUNT), \n",
    "    # where the first dimension represents the number of videos, the second dimension represents the \n",
    "    # number of frames per video, and the third dimension represents the number of distances per frame. \n",
    "    # You can reshape this data matrix to have the shape (total_samples, timesteps, features).\n",
    "    reshaped_data = np.array(input_set).reshape((-1, FRAMES_PER_SAMPLE, DISTANCES_COUNT))\n",
    "    print(f\"Reshaped Data: {reshaped_data.shape}\")\n",
    "\n",
    "    # Assuming 'labels' is a list or array containing the action labels for each video\n",
    "    one_hot_labels = to_categorical(label_per_video)\n",
    "    print(f\"\\nOne hot Labels:\\n{one_hot_labels}\")\n",
    "\n",
    "    print(\"Splitting Dataset...\")\n",
    "    # Divide the data set samples into 80:20 for train:test\n",
    "\n",
    "    # Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(reshaped_data, one_hot_labels, test_size=TEST_PERCENT, random_state=RANDOM_SEED)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=TEST_PERCENT, random_state=RANDOM_SEED)\n",
    "    # print(f\"Len of X_train: {len(X_train)}\\nLen of X_test: {len(X_test)}\\nLen of Y_train: {len(Y_train)}\\nLen of Y_test: {len(Y_test)}\\nLen of X_val: {len(X_val)}\\nLen of Y_val: {len(Y_val)}\")\n",
    "\n",
    "    print(\"Creating Simple LSTM Model...\")\n",
    "    # Training Phase\n",
    "    # Create a TF Sequential Model\n",
    "    model = Sequential()\n",
    "    # Add an LSTM Layer with 100 output space and input shape as FRAMES_PER_SAMPLExDISTANCES_COUNT (ref: https://sci-hub.se/https://doi.org/10.1109/CITISIA50690.2020.9371776)\n",
    "    model.add(LSTM(LSTM_OP_SPACE, input_shape=(FRAMES_PER_SAMPLE, DISTANCES_COUNT)))  # Adjust the number of units as needed\n",
    "    # Add a Dropout Layer with 100 output space\n",
    "    model.add(Dropout(DROPOUT_FACTOR))\n",
    "    # Add a Dense Layer with 100 output space\n",
    "    model.add(Dense(LSTM_OP_SPACE))\n",
    "    # Add a Dense Layer with STROKE_COUNT output space\n",
    "    model.add(Dense(STROKE_COUNT, activation='softmax'))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Fit the model with the current dataset with the xvalues and the labelvalues\n",
    "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=MODEL_EPOCH_COUNT, batch_size=MODEL_BATCH_SIZE)\n",
    "    # print(f\"\\nTensorflow Model:\\n{model}\")\n",
    "    print(f\"\\nModel Summary:\\n{model.summary()}\")\n",
    "\n",
    "\n",
    "    # The code commented below has not been tested to Optimize the model for phones\n",
    "    \"\"\"\n",
    "    if SHOULD_MOBILE_OPTIMIZE != 0:\n",
    "        from tensorflow.lite.python.util import run_graph_optimizations\n",
    "        from tensorflow.lite.python.convert import convert\n",
    "\n",
    "        # Apply model quantization\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Apply model compression techniques\n",
    "        # e.g. prune or quantize the model further\n",
    "\n",
    "        # Save the optimized model\n",
    "        with open('optimized_model.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        # Load and run the optimized model on mobile devices\n",
    "        interpreter = tf.lite.Interpreter(model_path='optimized_model.tflite')\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        # Prepare input data for inference\n",
    "        input_data = data.astype(np.float32)\n",
    "\n",
    "        # Run inference on the input data\n",
    "        interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n",
    "\n",
    "        # Evaluate the model accuracy\n",
    "        predicted_labels = np.argmax(output_data, axis=1)\n",
    "        accuracy = np.mean(predicted_labels == labels)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Testing Phase\n",
    "    # Call Predict function on the test array and get the label\n",
    "    # Test all the videos for testing and calculate the factors \"Precision\", \"Recall\", \"F1 Score\"\n",
    "        # Check this article for formulae: https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec\n",
    "\n",
    "    # Perform inference on testing data\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_test = np.argmax(Y_test, axis=1)\n",
    "    print(f\"\\ny_test:\\n{y_test}\")\n",
    "    # Convert predicted probabilities to class labels (if needed)\n",
    "    predicted_labels = np.argmax(Y_pred, axis=1)\n",
    "    print(f\"predicted_labels:\\n{predicted_labels}\")\n",
    "\n",
    "    report = classification_report(y_test, predicted_labels)\n",
    "    print(f\"\\nModel Report:\\n{report}\")\n",
    "\n",
    "    # Save the model as SavedModel Format\n",
    "    tf.saved_model.save(model, MODEL_NAME)\n",
    "\n",
    "    # To Save the model as HDF5\n",
    "    # model.save('your_model.h5')\n",
    "\n",
    "    print(f\"Model saved as {MODEL_NAME}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Keypoint files...\n",
      "File: videos/Forehand/output/0021.mp4.points\n",
      "File: videos/Forehand/output/0016.mp4.points\n",
      "File: videos/Forehand/output/0007.mp4.points\n",
      "File: videos/Forehand/output/0030.mp4.points\n",
      "File: videos/Forehand/output/0029.mp4.points\n",
      "File: videos/Forehand/output/0002.mp4.points\n",
      "File: videos/Forehand/output/0013.mp4.points\n",
      "File: videos/Forehand/output/0024.mp4.points\n",
      "File: videos/Forehand/output/0008.mp4.points\n",
      "File: videos/Forehand/output/0014.mp4.points\n",
      "File: videos/Forehand/output/0023.mp4.points\n",
      "File: videos/Forehand/output/0005.mp4.points\n",
      "File: videos/Forehand/output/0019.mp4.points\n",
      "File: videos/Forehand/output/0026.mp4.points\n",
      "File: videos/Forehand/output/0011.mp4.points\n",
      "File: videos/Forehand/output/0028.mp4.points\n",
      "File: videos/Forehand/output/0003.mp4.points\n",
      "File: videos/Forehand/output/0012.mp4.points\n",
      "File: videos/Forehand/output/0025.mp4.points\n",
      "File: videos/Forehand/output/0020.mp4.points\n",
      "File: videos/Forehand/output/0017.mp4.points\n",
      "File: videos/Forehand/output/0006.mp4.points\n",
      "File: videos/Forehand/output/0001.mp4.points\n",
      "File: videos/Forehand/output/0027.mp4.points\n",
      "File: videos/Forehand/output/0010.mp4.points\n",
      "File: videos/Forehand/output/0009.mp4.points\n",
      "File: videos/Forehand/output/0015.mp4.points\n",
      "File: videos/Forehand/output/0022.mp4.points\n",
      "File: videos/Forehand/output/0004.mp4.points\n",
      "File: videos/Forehand/output/0018.mp4.points\n",
      "File: videos/Backhand/output/0021.mp4.points\n",
      "File: videos/Backhand/output/0016.mp4.points\n",
      "File: videos/Backhand/output/0007.mp4.points\n",
      "File: videos/Backhand/output/0030.mp4.points\n",
      "File: videos/Backhand/output/0029.mp4.points\n",
      "File: videos/Backhand/output/0002.mp4.points\n",
      "File: videos/Backhand/output/0013.mp4.points\n",
      "File: videos/Backhand/output/0024.mp4.points\n",
      "File: videos/Backhand/output/0008.mp4.points\n",
      "File: videos/Backhand/output/0014.mp4.points\n",
      "File: videos/Backhand/output/0023.mp4.points\n",
      "File: videos/Backhand/output/0005.mp4.points\n",
      "File: videos/Backhand/output/0019.mp4.points\n",
      "File: videos/Backhand/output/0026.mp4.points\n",
      "File: videos/Backhand/output/0011.mp4.points\n",
      "File: videos/Backhand/output/0028.mp4.points\n",
      "File: videos/Backhand/output/0003.mp4.points\n",
      "File: videos/Backhand/output/0012.mp4.points\n",
      "File: videos/Backhand/output/0025.mp4.points\n",
      "File: videos/Backhand/output/0020.mp4.points\n",
      "File: videos/Backhand/output/0017.mp4.points\n",
      "File: videos/Backhand/output/0006.mp4.points\n",
      "File: videos/Backhand/output/0001.mp4.points\n",
      "File: videos/Backhand/output/0027.mp4.points\n",
      "File: videos/Backhand/output/0010.mp4.points\n",
      "File: videos/Backhand/output/0009.mp4.points\n",
      "File: videos/Backhand/output/0015.mp4.points\n",
      "File: videos/Backhand/output/0022.mp4.points\n",
      "File: videos/Backhand/output/0004.mp4.points\n",
      "File: videos/Backhand/output/0018.mp4.points\n",
      "File: videos/NoStroke/output/0021.mp4.points\n",
      "File: videos/NoStroke/output/0016.mp4.points\n",
      "File: videos/NoStroke/output/0043.mp4.points\n",
      "File: videos/NoStroke/output/0052.mp4.points\n",
      "File: videos/NoStroke/output/0007.mp4.points\n",
      "File: videos/NoStroke/output/0030.mp4.points\n",
      "File: videos/NoStroke/output/0029.mp4.points\n",
      "File: videos/NoStroke/output/0057.mp4.points\n",
      "File: videos/NoStroke/output/0060.mp4.points\n",
      "File: videos/NoStroke/output/0035.mp4.points\n",
      "File: videos/NoStroke/output/0002.mp4.points\n",
      "File: videos/NoStroke/output/0013.mp4.points\n",
      "File: videos/NoStroke/output/0024.mp4.points\n",
      "File: videos/NoStroke/output/0046.mp4.points\n",
      "File: videos/NoStroke/output/0038.mp4.points\n",
      "File: videos/NoStroke/output/0008.mp4.points\n",
      "File: videos/NoStroke/output/0014.mp4.points\n",
      "File: videos/NoStroke/output/0023.mp4.points\n",
      "File: videos/NoStroke/output/0041.mp4.points\n",
      "File: videos/NoStroke/output/0050.mp4.points\n",
      "File: videos/NoStroke/output/0032.mp4.points\n",
      "File: videos/NoStroke/output/0005.mp4.points\n",
      "File: videos/NoStroke/output/0019.mp4.points\n",
      "File: videos/NoStroke/output/0055.mp4.points\n",
      "File: videos/NoStroke/output/0037.mp4.points\n",
      "File: videos/NoStroke/output/0049.mp4.points\n",
      "File: videos/NoStroke/output/0058.mp4.points\n",
      "File: videos/NoStroke/output/0026.mp4.points\n",
      "File: videos/NoStroke/output/0011.mp4.points\n",
      "File: videos/NoStroke/output/0044.mp4.points\n",
      "File: videos/NoStroke/output/0028.mp4.points\n",
      "File: videos/NoStroke/output/0056.mp4.points\n",
      "File: videos/NoStroke/output/0034.mp4.points\n",
      "File: videos/NoStroke/output/0003.mp4.points\n",
      "File: videos/NoStroke/output/0012.mp4.points\n",
      "File: videos/NoStroke/output/0025.mp4.points\n",
      "File: videos/NoStroke/output/0047.mp4.points\n",
      "File: videos/NoStroke/output/0039.mp4.points\n",
      "File: videos/NoStroke/output/0020.mp4.points\n",
      "File: videos/NoStroke/output/0017.mp4.points\n",
      "File: videos/NoStroke/output/0042.mp4.points\n",
      "File: videos/NoStroke/output/0053.mp4.points\n",
      "File: videos/NoStroke/output/0006.mp4.points\n",
      "File: videos/NoStroke/output/0031.mp4.points\n",
      "File: videos/NoStroke/output/0054.mp4.points\n",
      "File: videos/NoStroke/output/0001.mp4.points\n",
      "File: videos/NoStroke/output/0036.mp4.points\n",
      "File: videos/NoStroke/output/0048.mp4.points\n",
      "File: videos/NoStroke/output/0059.mp4.points\n",
      "File: videos/NoStroke/output/0027.mp4.points\n",
      "File: videos/NoStroke/output/0010.mp4.points\n",
      "File: videos/NoStroke/output/0045.mp4.points\n",
      "File: videos/NoStroke/output/0009.mp4.points\n",
      "File: videos/NoStroke/output/0015.mp4.points\n",
      "File: videos/NoStroke/output/0022.mp4.points\n",
      "File: videos/NoStroke/output/0040.mp4.points\n",
      "File: videos/NoStroke/output/0051.mp4.points\n",
      "File: videos/NoStroke/output/0033.mp4.points\n",
      "File: videos/NoStroke/output/0004.mp4.points\n",
      "File: videos/NoStroke/output/0018.mp4.points\n",
      "Calculating Distances...\n",
      "\n",
      "Saved graph: images/Forehand_video_1_distances.png\n",
      "Saved graph: images/Forehand_video_2_distances.png\n",
      "Saved graph: images/Forehand_video_3_distances.png\n",
      "Saved graph: images/Forehand_video_4_distances.png\n",
      "Saved graph: images/Forehand_video_5_distances.png\n",
      "Saved graph: images/Forehand_video_6_distances.png\n",
      "Saved graph: images/Forehand_video_7_distances.png\n",
      "Saved graph: images/Forehand_video_8_distances.png\n",
      "Saved graph: images/Forehand_video_9_distances.png\n",
      "Saved graph: images/Forehand_video_10_distances.png\n",
      "Saved graph: images/Forehand_video_11_distances.png\n",
      "Saved graph: images/Forehand_video_12_distances.png\n",
      "Saved graph: images/Forehand_video_13_distances.png\n",
      "Saved graph: images/Forehand_video_14_distances.png\n",
      "Saved graph: images/Forehand_video_15_distances.png\n",
      "Saved graph: images/Forehand_video_16_distances.png\n",
      "Saved graph: images/Forehand_video_17_distances.png\n",
      "Saved graph: images/Forehand_video_18_distances.png\n",
      "Saved graph: images/Forehand_video_19_distances.png\n",
      "Saved graph: images/Forehand_video_20_distances.png\n",
      "Saved graph: images/Forehand_video_21_distances.png\n",
      "Saved graph: images/Forehand_video_22_distances.png\n",
      "Saved graph: images/Forehand_video_23_distances.png\n",
      "Saved graph: images/Forehand_video_24_distances.png\n",
      "Saved graph: images/Forehand_video_25_distances.png\n",
      "Saved graph: images/Forehand_video_26_distances.png\n",
      "Saved graph: images/Forehand_video_27_distances.png\n",
      "Saved graph: images/Forehand_video_28_distances.png\n",
      "Saved graph: images/Forehand_video_29_distances.png\n",
      "Saved graph: images/Forehand_video_30_distances.png\n",
      "Saved graph: images/Backhand_video_1_distances.png\n",
      "Saved graph: images/Backhand_video_2_distances.png\n",
      "Saved graph: images/Backhand_video_3_distances.png\n",
      "Saved graph: images/Backhand_video_4_distances.png\n",
      "Saved graph: images/Backhand_video_5_distances.png\n",
      "Saved graph: images/Backhand_video_6_distances.png\n",
      "Saved graph: images/Backhand_video_7_distances.png\n",
      "Saved graph: images/Backhand_video_8_distances.png\n",
      "Saved graph: images/Backhand_video_9_distances.png\n",
      "Saved graph: images/Backhand_video_10_distances.png\n",
      "Saved graph: images/Backhand_video_11_distances.png\n",
      "Saved graph: images/Backhand_video_12_distances.png\n",
      "Saved graph: images/Backhand_video_13_distances.png\n",
      "Saved graph: images/Backhand_video_14_distances.png\n",
      "Saved graph: images/Backhand_video_15_distances.png\n",
      "Saved graph: images/Backhand_video_16_distances.png\n",
      "Saved graph: images/Backhand_video_17_distances.png\n",
      "Saved graph: images/Backhand_video_18_distances.png\n",
      "Saved graph: images/Backhand_video_19_distances.png\n",
      "Saved graph: images/Backhand_video_20_distances.png\n",
      "Saved graph: images/Backhand_video_21_distances.png\n",
      "Saved graph: images/Backhand_video_22_distances.png\n",
      "Saved graph: images/Backhand_video_23_distances.png\n",
      "Saved graph: images/Backhand_video_24_distances.png\n",
      "Saved graph: images/Backhand_video_25_distances.png\n",
      "Saved graph: images/Backhand_video_26_distances.png\n",
      "Saved graph: images/Backhand_video_27_distances.png\n",
      "Saved graph: images/Backhand_video_28_distances.png\n",
      "Saved graph: images/Backhand_video_29_distances.png\n",
      "Saved graph: images/Backhand_video_30_distances.png\n",
      "Saved graph: images/NoStroke_video_1_distances.png\n",
      "Saved graph: images/NoStroke_video_2_distances.png\n",
      "Saved graph: images/NoStroke_video_3_distances.png\n",
      "Saved graph: images/NoStroke_video_4_distances.png\n",
      "Saved graph: images/NoStroke_video_5_distances.png\n",
      "Saved graph: images/NoStroke_video_6_distances.png\n",
      "Saved graph: images/NoStroke_video_7_distances.png\n",
      "Saved graph: images/NoStroke_video_8_distances.png\n",
      "Saved graph: images/NoStroke_video_9_distances.png\n",
      "Saved graph: images/NoStroke_video_10_distances.png\n",
      "Saved graph: images/NoStroke_video_11_distances.png\n",
      "Saved graph: images/NoStroke_video_12_distances.png\n",
      "Saved graph: images/NoStroke_video_13_distances.png\n",
      "Saved graph: images/NoStroke_video_14_distances.png\n",
      "Saved graph: images/NoStroke_video_15_distances.png\n",
      "Saved graph: images/NoStroke_video_16_distances.png\n",
      "Saved graph: images/NoStroke_video_17_distances.png\n",
      "Saved graph: images/NoStroke_video_18_distances.png\n",
      "Saved graph: images/NoStroke_video_19_distances.png\n",
      "Saved graph: images/NoStroke_video_20_distances.png\n",
      "Saved graph: images/NoStroke_video_21_distances.png\n",
      "Saved graph: images/NoStroke_video_22_distances.png\n",
      "Saved graph: images/NoStroke_video_23_distances.png\n",
      "Saved graph: images/NoStroke_video_24_distances.png\n",
      "Saved graph: images/NoStroke_video_25_distances.png\n",
      "Saved graph: images/NoStroke_video_26_distances.png\n",
      "Saved graph: images/NoStroke_video_27_distances.png\n",
      "Saved graph: images/NoStroke_video_28_distances.png\n",
      "Saved graph: images/NoStroke_video_29_distances.png\n",
      "Saved graph: images/NoStroke_video_30_distances.png\n",
      "Saved graph: images/NoStroke_video_31_distances.png\n",
      "Saved graph: images/NoStroke_video_32_distances.png\n",
      "Saved graph: images/NoStroke_video_33_distances.png\n",
      "Saved graph: images/NoStroke_video_34_distances.png\n",
      "Saved graph: images/NoStroke_video_35_distances.png\n",
      "Saved graph: images/NoStroke_video_36_distances.png\n",
      "Saved graph: images/NoStroke_video_37_distances.png\n",
      "Saved graph: images/NoStroke_video_38_distances.png\n",
      "Saved graph: images/NoStroke_video_39_distances.png\n",
      "Saved graph: images/NoStroke_video_40_distances.png\n",
      "Saved graph: images/NoStroke_video_41_distances.png\n",
      "Saved graph: images/NoStroke_video_42_distances.png\n",
      "Saved graph: images/NoStroke_video_43_distances.png\n",
      "Saved graph: images/NoStroke_video_44_distances.png\n",
      "Saved graph: images/NoStroke_video_45_distances.png\n",
      "Saved graph: images/NoStroke_video_46_distances.png\n",
      "Saved graph: images/NoStroke_video_47_distances.png\n",
      "Saved graph: images/NoStroke_video_48_distances.png\n",
      "Saved graph: images/NoStroke_video_49_distances.png\n",
      "Saved graph: images/NoStroke_video_50_distances.png\n",
      "Saved graph: images/NoStroke_video_51_distances.png\n",
      "Saved graph: images/NoStroke_video_52_distances.png\n",
      "Saved graph: images/NoStroke_video_53_distances.png\n",
      "Saved graph: images/NoStroke_video_54_distances.png\n",
      "Saved graph: images/NoStroke_video_55_distances.png\n",
      "Saved graph: images/NoStroke_video_56_distances.png\n",
      "Saved graph: images/NoStroke_video_57_distances.png\n",
      "Saved graph: images/NoStroke_video_58_distances.png\n",
      "Saved graph: images/NoStroke_video_59_distances.png\n",
      "Saved graph: images/NoStroke_video_60_distances.png\n",
      "Reshaped Data: (120, 58, 9)\n",
      "\n",
      "One hot Labels:\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "Splitting Dataset...\n",
      "Creating Simple LSTM Model...\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 145ms/step - loss: 1.3146 - accuracy: 0.3289 - val_loss: 1.2744 - val_accuracy: 0.3000\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0146 - accuracy: 0.5658 - val_loss: 1.4337 - val_accuracy: 0.3000\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.0137 - accuracy: 0.5658 - val_loss: 1.2746 - val_accuracy: 0.3000\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.9075 - accuracy: 0.6184 - val_loss: 1.1095 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.9552 - accuracy: 0.5263 - val_loss: 1.0804 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.8778 - accuracy: 0.6316 - val_loss: 1.0725 - val_accuracy: 0.4500\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.8211 - accuracy: 0.6316 - val_loss: 1.1630 - val_accuracy: 0.3000\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.8228 - accuracy: 0.5921 - val_loss: 0.9518 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.7110 - accuracy: 0.7500 - val_loss: 0.8237 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6118 - accuracy: 0.7500 - val_loss: 0.9157 - val_accuracy: 0.5500\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5373 - accuracy: 0.7763 - val_loss: 0.6317 - val_accuracy: 0.5500\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4334 - accuracy: 0.8026 - val_loss: 0.6519 - val_accuracy: 0.5500\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4758 - accuracy: 0.7895 - val_loss: 0.3234 - val_accuracy: 0.9000\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3825 - accuracy: 0.8289 - val_loss: 0.2524 - val_accuracy: 0.9000\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4152 - accuracy: 0.7895 - val_loss: 1.1388 - val_accuracy: 0.4000\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5443 - accuracy: 0.7763 - val_loss: 0.4174 - val_accuracy: 0.8500\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6372 - accuracy: 0.7368 - val_loss: 0.4271 - val_accuracy: 0.8000\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3940 - accuracy: 0.8421 - val_loss: 0.6763 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4757 - accuracy: 0.7763 - val_loss: 0.7048 - val_accuracy: 0.5500\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3991 - accuracy: 0.7895 - val_loss: 0.3855 - val_accuracy: 0.8500\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2932 - accuracy: 0.8684 - val_loss: 0.2733 - val_accuracy: 0.9000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3050 - accuracy: 0.9079 - val_loss: 0.2380 - val_accuracy: 0.9000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3058 - accuracy: 0.8684 - val_loss: 0.2818 - val_accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2765 - accuracy: 0.8684 - val_loss: 0.3708 - val_accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2530 - accuracy: 0.8947 - val_loss: 0.3011 - val_accuracy: 0.8500\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2207 - accuracy: 0.9211 - val_loss: 0.2314 - val_accuracy: 0.9500\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2272 - accuracy: 0.9079 - val_loss: 0.1726 - val_accuracy: 0.9500\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2155 - accuracy: 0.9342 - val_loss: 0.2060 - val_accuracy: 0.9500\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1437 - accuracy: 0.9474 - val_loss: 0.1796 - val_accuracy: 0.9500\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1439 - accuracy: 0.9605 - val_loss: 0.1292 - val_accuracy: 0.9500\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1479 - accuracy: 0.9737 - val_loss: 0.0771 - val_accuracy: 0.9500\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2061 - accuracy: 0.8947 - val_loss: 0.0755 - val_accuracy: 0.9500\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2050 - accuracy: 0.9474 - val_loss: 0.1939 - val_accuracy: 0.8500\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1502 - accuracy: 0.9342 - val_loss: 1.1209 - val_accuracy: 0.7000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2568 - accuracy: 0.8816 - val_loss: 0.0567 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2878 - accuracy: 0.9211 - val_loss: 0.2351 - val_accuracy: 0.9000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2680 - accuracy: 0.9079 - val_loss: 0.5542 - val_accuracy: 0.8500\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2856 - accuracy: 0.9079 - val_loss: 0.3514 - val_accuracy: 0.8500\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4655 - accuracy: 0.8158 - val_loss: 0.1956 - val_accuracy: 0.9500\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2521 - accuracy: 0.8947 - val_loss: 0.5479 - val_accuracy: 0.7000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1838 - accuracy: 0.8816 - val_loss: 0.4545 - val_accuracy: 0.8000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2211 - accuracy: 0.9211 - val_loss: 0.3859 - val_accuracy: 0.8500\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2352 - accuracy: 0.9079 - val_loss: 0.5414 - val_accuracy: 0.8000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1753 - accuracy: 0.9342 - val_loss: 0.7182 - val_accuracy: 0.8000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2188 - accuracy: 0.8947 - val_loss: 0.0874 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1563 - accuracy: 0.9605 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2042 - accuracy: 0.9474 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1545 - accuracy: 0.9342 - val_loss: 0.0885 - val_accuracy: 0.9500\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1200 - accuracy: 0.9474 - val_loss: 0.1506 - val_accuracy: 0.9500\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0888 - accuracy: 0.9868 - val_loss: 0.2545 - val_accuracy: 0.9500\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 100)               44000     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54504 (212.91 KB)\n",
      "Trainable params: 54504 (212.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Model Summary:\n",
      "None\n",
      "1/1 [==============================] - 1s 503ms/step\n",
      "\n",
      "y_test:\n",
      "[2 2 1 2 1 3 3 1 2 3 1 3 1 2 3 3 3 1 3 3 3 2 2 3]\n",
      "predicted_labels:\n",
      "[2 2 1 2 1 3 3 1 2 3 1 3 1 2 3 2 3 1 3 3 3 2 2 3]\n",
      "\n",
      "Model Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.88      1.00      0.93         7\n",
      "           3       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.96        24\n",
      "   macro avg       0.96      0.97      0.96        24\n",
      "weighted avg       0.96      0.96      0.96        24\n",
      "\n",
      "INFO:tensorflow:Assets written to: models/tf_simple_lstm_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/tf_simple_lstm_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as models/tf_simple_lstm_model...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
